{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "import os\n",
    "from data import utils as CTRUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU, Input\n",
    "from keras.layers import Reshape, UpSampling2D, MaxPooling2D\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Flatten\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.datasets import mnist\n",
    "import keras\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "JSRT_DIR = DATA_DIR + \"jsrt/\"\n",
    "WING_DIR = DATA_DIR + \"wingspan/\"\n",
    "JSRT_FNAMES = os.listdir(JSRT_DIR + \"png\")\n",
    "WING_FNAMES = os.listdir(WING_DIR + \"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imgs(base_dir, fname):\n",
    "    \"\"\"\n",
    "    Given base_dir = JSRT_DIR or WING_DIR,\n",
    "    Returns tuple of numpy arrays representing \n",
    "        - Original scan\n",
    "        - Left lung mask\n",
    "        - Right lung mask\n",
    "        - Heart mask\n",
    "    \"\"\"\n",
    "    img = skimage.io.imread(base_dir + \"png/\" + fname)\n",
    "    left = skimage.io.imread(base_dir + \"mask/left_lung/\" + fname)\n",
    "    right = skimage.io.imread(base_dir + \"mask/right_lung/\" + fname)\n",
    "    heart = skimage.io.imread(base_dir + \"mask/heart/\" + fname)\n",
    "    for idx, mask in enumerate([left, right, heart]):\n",
    "        mask[mask > 0] = idx + 1\n",
    "    return img, left, right, heart\n",
    "\n",
    "def print_img_demo(n_imgs = 3):\n",
    "    \"\"\"\n",
    "    Prints some JSRT images with overlays (just for looking at dataset.)\n",
    "    Can pass in n_imgs = (int) x to print x images. \n",
    "    \"\"\"\n",
    "    if n_imgs > len(JSRT_FNAMES):\n",
    "        n_imgs = len(JSRT_FNAMES)\n",
    "    for frame in JSRT_FNAMES[:n_imgs]:\n",
    "        img, left, right, heart = get_imgs(JSRT_DIR, frame)\n",
    "        show_annotation(img, left, right, heart)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def get_input_image_example(base_dir, fname):\n",
    "    \"\"\"\n",
    "    Parameter\n",
    "        base_dir = JSRT_DIR or WING_DIR,\n",
    "        fname = one of JSRT_FNAMES or WING_FNAMES\n",
    "    Returns numpy array representing image with 1 BW channel.\n",
    "    Range of each value is [0.0, 1.0]\n",
    "    Shape: (height, width, 1 channel)\n",
    "    \"\"\"\n",
    "    img = skimage.io.imread(base_dir + \"png/\" + fname)\n",
    "    norm = img / 255.0\n",
    "    return np.expand_dims(norm, -1)\n",
    "\n",
    "\n",
    "def get_input_image_examples(base_dir, fnames):\n",
    "    \"\"\"\n",
    "    Accumulates get_input_image_example. \n",
    "    Shape: (#examples, height, width, 1 channel)\n",
    "    \"\"\"\n",
    "    return np.array([\n",
    "        get_input_image_example(base_dir, fname)\n",
    "        for fname in fnames\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_ground_truth_example(base_dir, fname):\n",
    "    \"\"\"\n",
    "    Parameter\n",
    "        base_dir = JSRT_DIR or WING_DIR,\n",
    "        fname = one of JSRT_FNAMES or WING_FNAMES\n",
    "    Returns numpy array representing image with 4 channels (one-hot):\n",
    "        0: none\n",
    "        1: Left lung mask\n",
    "        2: Right lung mask\n",
    "        3: Heart mask\n",
    "    Shape: (height, width, 4 channels)\n",
    "    \"\"\"\n",
    "    left = skimage.io.imread(base_dir + \"mask/left_lung/\" + fname)\n",
    "    right = skimage.io.imread(base_dir + \"mask/right_lung/\" + fname)\n",
    "    heart = skimage.io.imread(base_dir + \"mask/heart/\" + fname)\n",
    "\n",
    "    for mask in (left, right, heart):\n",
    "        mask[mask > 0] = 1.0\n",
    "        \n",
    "    non = np.ones(left.shape) - left - right - heart\n",
    "    non[non < 0] = 0.0\n",
    "    non[non > 0] = 1.0\n",
    "    \n",
    "    concat = np.stack((non, left, right, heart), axis=2)\n",
    "    return concat\n",
    "\n",
    "def get_ground_truth_set(base_dir, fnames):\n",
    "    \"\"\"\n",
    "    Accumulates get_ground_truth_example. \n",
    "    Shape: (#examples, height, width, 4 channels)\n",
    "    \"\"\"\n",
    "    return np.array([\n",
    "        get_ground_truth_example(base_dir, fname)\n",
    "        for fname in fnames\n",
    "    ])\n",
    "\n",
    "def get_data(base_dir, fnames):\n",
    "    \"\"\"\n",
    "    Calls get_input_image_examples on the given \n",
    "        base_dir : ex. JSRT_DIR\n",
    "        fnames : ex. JSRT_FNAMES\n",
    "    \"\"\"\n",
    "    X = get_input_image_examples(base_dir, fnames)\n",
    "    Y = get_ground_truth_set(base_dir, fnames)\n",
    "    return X, Y\n",
    "\n",
    "def show_annotation(img, left, right, heart):\n",
    "    annotated = CTRUtil.add_seg(img, left + right + heart)\n",
    "    skimage.io.imshow(annotated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((247, 512, 512, 1), (247, 512, 512, 4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = get_data(JSRT_DIR, JSRT_FNAMES)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each x: (examples, H, W, 1 channel)\n",
    "Each y: (examples, H, W, 4 channels (left, right, heart, none))\n",
    "\n",
    "Discriminator uses Avg pooling, 1x1 conv. \n",
    "\n",
    "\n",
    "Strategy: First approach, only train on JSRT. Then, see how well it does on Wingspan (in terms of segmentation and especially in terms of CTR estimation). Idea: these __are__ two different datasets. Want to see if that makes an impact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_data() missing 1 required positional argument: 'fnames'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8b12f7fd2bf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-8b12f7fd2bf7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, path)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Import the MNIST dataset using Keras, will only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# use the 60,000 training examples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_data() missing 1 required positional argument: 'fnames'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Some constants\n",
    "MNIST_SIZE = 512\n",
    "LATENT_DIM = 100\n",
    "\n",
    "\n",
    "def make_generator(num_filters=64, num_hidden_conv_layers=2, init_dim=7):\n",
    "    gen = Sequential()\n",
    "    # Model input is a feature vector of size 100\n",
    "    gen.add(Dense(init_dim**2 * num_filters, input_dim=LATENT_DIM))\n",
    "    gen.add(Activation('relu'))\n",
    "    gen.add(Reshape((init_dim, init_dim, num_filters)))\n",
    "\n",
    "    for _ in range(num_hidden_conv_layers):\n",
    "        # Input: d x d x k\n",
    "        # Output 2d x 2d x k/2\n",
    "        if (init_dim < MNIST_SIZE):\n",
    "            gen.add(UpSampling2D())\n",
    "            init_dim *= 2\n",
    "        num_filters //= 2\n",
    "        gen.add(Conv2DTranspose(num_filters, 5, padding='same'))\n",
    "        gen.add(BatchNormalization(momentum=0.4))\n",
    "        gen.add(Activation('relu'))\n",
    "\n",
    "    gen.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "    gen.add(Activation('sigmoid'))\n",
    "    # Output should be 28 x 28 x 1\n",
    "    # gen.summary()\n",
    "    return gen\n",
    "\n",
    "\n",
    "def make_discriminator(num_filters=32, num_hidden_layers=3, dropout=0.3):\n",
    "    d = Sequential()\n",
    "\n",
    "    d.add(Conv2D(num_filters*1, 5, strides=2,\n",
    "                 input_shape=(MNIST_SIZE, MNIST_SIZE, 1), padding='same'))\n",
    "    d.add(LeakyReLU())  # leakyrelu so generator has derivative\n",
    "    d.add(Dropout(dropout))\n",
    "\n",
    "    for i in range(1, num_hidden_layers):\n",
    "        # Powers of 2 are generally better suited for GPU\n",
    "        d.add(Conv2D(num_filters*(2**i), 5, strides=2, padding='same'))\n",
    "        d.add(LeakyReLU())\n",
    "        d.add(Dropout(dropout))\n",
    "\n",
    "    # NOTE: Difference between this and build_conv_net\n",
    "    #       is that there is only a SINGLE output class,\n",
    "    #       which corresponds to FAKE/REAL.\n",
    "    d.add(Flatten())\n",
    "    d.add(Dense(1))\n",
    "    d.add(Activation('sigmoid'))\n",
    "    d.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return d\n",
    "\n",
    "\n",
    "def make_adversial_network(generator, discriminator):\n",
    "    # This will only be used for training the generator.\n",
    "    # Note, the weights in the discriminator and generator are shared.\n",
    "    discriminator.trainable = False\n",
    "    gan = Sequential([generator, discriminator])\n",
    "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return gan  # , generator, discriminator\n",
    "\n",
    "\n",
    "def generate_latent_noise(n):\n",
    "    return np.random.uniform(-1, 1, size=(n, LATENT_DIM))\n",
    "\n",
    "\n",
    "def visualize_generator(epoch, generator,\n",
    "                        num_samples=100, dim=(10, 10),\n",
    "                        figsize=(10, 10), path=''):\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        img = generator.predict(generate_latent_noise(1))[0, :, :, 0]\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'generator_samples/gan_epoch_{epoch}.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def train(epochs=1, batch_size=128, path=''):\n",
    "    # Grab all training examples. \n",
    "    X_images = X\n",
    "    X_segments = Y\n",
    "\n",
    "\n",
    "    # Creating GAN\n",
    "    generator = make_generator()\n",
    "    discriminator = make_discriminator()\n",
    "    adversial_net = make_adversial_network(generator, discriminator)\n",
    "\n",
    "    visualize_generator(0, generator, path=path)\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "\n",
    "        discr_loss = 0\n",
    "        gen_loss = 0\n",
    "        for _ in tqdm(range(batch_size)):\n",
    "            noise = generate_latent_noise(batch_size)\n",
    "            generated_segments = generator.predict(noise)\n",
    "\n",
    "            rand_choice = np.random.choice(X_segments.shape[0], batch_size,\n",
    "                                                   replace=False)\n",
    "            real_images = X_images[rand_choice]\n",
    "            real_segments = X_segments[rand_choice]\n",
    "            discrimination_data = np.concatenate([real_segments, generated_segments])\n",
    "\n",
    "            # Labels for generated and real data, uses soft label trick\n",
    "            discrimination_labels = 0.1 * np.ones(2 * batch_size)\n",
    "            discrimination_labels[:batch_size] = 0.9\n",
    "\n",
    "            # To train, we alternate between training just the discriminator\n",
    "            # and just the generator.\n",
    "            discriminator.trainable = True\n",
    "            discr_loss += discriminator.train_on_batch(discrimination_data,\n",
    "                                                       discrimination_labels)\n",
    "\n",
    "            # Trick to 'freeze' discriminator weights in adversial_net. Only\n",
    "            # the generator weights will be changed, which are shared with\n",
    "            # the generator.\n",
    "            discriminator.trainable = False\n",
    "            # N.B, changing the labels because now we want to 'fool' the\n",
    "            # discriminator.\n",
    "            gen_loss += adversial_net.train_on_batch(\n",
    "                noise, np.ones(batch_size))\n",
    "\n",
    "        print(f'Discriminator Loss: {discr_loss/batch_size}')\n",
    "        print(f'Generator Loss:     {gen_loss/batch_size}')\n",
    "        visualize_generator(epoch+1, generator, path=path)\n",
    "\n",
    "\n",
    "train(epochs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
